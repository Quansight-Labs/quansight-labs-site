<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quansight Labs (Posts about Open-Source)</title><link>https://labs.quansight.org/</link><description></description><atom:link href="https://labs.quansight.org/categories/open-source.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:info@quansight.com"&gt;Quansight Labs Team&lt;/a&gt; </copyright><lastBuildDate>Mon, 12 Jul 2021 10:47:33 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Rethinking Jupyter Interactive Documentation</title><link>https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/</link><dc:creator>Matthias Bussonnier</dc:creator><description>&lt;div&gt;&lt;p&gt;Jupyter Notebook first release was 8 years ago – under the IPython Notebook
name at the time. Even if notebooks were not invented by Jupyter; they were
definitely democratized by it. Being Web powered allowed development of many
changes in the Datascience world. Objects now often expose rich representation; from
Pandas dataframes with as html tables, to more recent &lt;a href="https://github.com/scikit-learn/scikit-learn/pull/14180"&gt;Scikit-learn model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Today I want to look into a topic that has not evolved much since, and I believe
could use an upgrade. Accessing interactive Documentation when in a Jupyter
session, and what it could become. At the end I'll link to my current prototype
if you are adventurous.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>documentation</category><category>Open-Source</category><category>Python</category><guid>https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/</guid><pubDate>Fri, 07 May 2021 00:01:00 GMT</pubDate></item><item><title>Making SciPy's Image Interpolation Consistent and Well Documented</title><link>https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/</link><dc:creator>Gregory Lee</dc:creator><description>&lt;div&gt;&lt;h2&gt;SciPy n-dimensional Image Processing&lt;/h2&gt;
&lt;p&gt;SciPy's ndimage module provides a powerful set of general, n-dimensional image processing operations, categorized into areas such as filtering, interpolation and morphology. Traditional image processing deals with 2D arrays of pixels, possibly with an additional array dimension of size 3 or 4 to represent color channel and transparency information. However, there are many scientific applications where we may want to work with more general arrays such as the 3D volumetric images produced by medical imaging methods like computed tomography (CT) or magnetic resonance imaging (MRI) or biological imaging approaches such as light sheet microscopy. Aside from spatial axes, such data may have additional axes representing other quantities such as time, color, spectral frequency or different contrasts. Functions in ndimage have been implemented in a general n-dimensional manner so that they can be applied across 2D, 3D or more dimensions. A more detailed overview of the module is available in the
&lt;a href="https://docs.scipy.org/doc/scipy/reference/tutorial/ndimage.html"&gt;SciPy ndimage tutorial&lt;/a&gt;. SciPy's image functions are also used by downstream libraries such as &lt;a href="https://scikit-image.org"&gt;scikit-image&lt;/a&gt; to implement higher-level algorithms for things like image restoration, segmentation and registration.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/"&gt;Read more…&lt;/a&gt; (5 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Labs</category><category>Open-Source</category><category>Python</category><category>SciPy</category><guid>https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/</guid><pubDate>Fri, 22 Jan 2021 14:00:00 GMT</pubDate></item><item><title>Introduction to Design in Open Source</title><link>https://labs.quansight.org/blog/2020/11/introduction-to-design-in-open-source/</link><dc:creator>Tim George, Isabela Presedo-Floyd</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;em&gt;This blog post is a conversation. Portions lead by Tim George are marked with
&lt;/em&gt;&lt;em&gt;TG&lt;/em&gt;&lt;em&gt;, and those lead by Isabela Presedo-Floyd are marked with &lt;/em&gt;&lt;em&gt;IPF&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TG:&lt;/strong&gt; When I speak with other designers, one common theme I see concerning why
they chose this career path is they want to make a difference in the world. We
design because we imagine a better world and we want to help make it real. Part
of the reason we design as a career is we're unable to go through life without
designing; we're always thinking about how things are and how they could be
better. This ethos also exists in many open-source communities. It seems like it
ought to be an ideal match.&lt;/p&gt;
&lt;p&gt;So what's the disconnect? I'm still exploring that myself, but after a few years
in open source I want to share my observations, experiences, and hope for a
stronger collaboration between design and development. I don't think I have a
complete solution, and some days I'm not even sure I grasp the entire problem.
What I hope is to say that which often goes unsaid in these spaces: design and
development skills in open source coexist precariously.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2020/11/introduction-to-design-in-open-source/"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>design</category><category>Open-Source</category><category>User Experience</category><category>UX</category><guid>https://labs.quansight.org/blog/2020/11/introduction-to-design-in-open-source/</guid><pubDate>Wed, 18 Nov 2020 05:00:30 GMT</pubDate></item></channel></rss>