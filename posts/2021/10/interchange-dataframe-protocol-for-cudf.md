<!--
.. title: Interchange dataframe protocol: cuDF implementation
.. slug: data-apis-cudf
.. date: 2021-10-05 16:10:00 UTC+00:00
.. author: Ismaël Koné
.. tags: dataframe protocol, cuDF
.. category:
.. link:
.. description:
.. type: text
-->

Hey there, 

this Ismaël Koné from Côte d'Ivoire (Ivory Coast). I am a fan of open source software. 
In the next lines, I'll try to capture my experience at Quansight Labs as an intern working on the `cuDF` implementation of an interchange dataframe protocol.

## Presentation of the work to be done

Let's start by motivating this project through details about: **cuDF** and the **interchange dataframe protocol**.

<!-- TEASER_END -->


### cuDF

This is a dataframe library very much like `pandas` which operates on the GPU in order to benefit from its computing power. For more details about cuDF, please take a look at: [https://rapids.ai/](https://rapids.ai/) and [https://github.com/rapidsai/cudf](https://github.com/rapidsai/cudf).

### Motivations for the Interchange dataframe protocol
Let's start by a concrete use case. To set the stage, recall that there are many dataframe libraries out there: [pandas](https://pandas.pydata.org/), [vaex](https://vaex.io/), [modin](https://modin.readthedocs.io/en/latest/), [dask](https://dask.org/)/[cudf-dask](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html), etc... Each one has its strengths and weaknesses. For example, vaex allows you to work with bigger than memory (RAM) datasets on a laptop, dask allows you to distribute computation across processes and cluster nodes and cudf-dask is its GPU counterpart.
Suppose you have a **300 GB** datasets on your laptop and want to get some insights about it. A typical workflow can be:
```python
import vaex
# run some exploratory data analysis (EDA)
import dask, cudf
# take a sample of 3 GB
# run some operations with dask, then cudf and compare performance. 
```

Along the way, we should load the dataset many times. Or more commonly, we use the pair `to_pandas`/`from_pandas` to move from one dataframe library to another. Outcomes of these practices are:

- possible memory overhead

- no guarantee that all features are preserved during the conversion from/to pandas

- high coupling with `pandas` that breaks an important software design pattern: [Dependency Inversion Principle (DIP)](https://en.wikipedia.org/wiki/Dependency_inversion_principle) which promotes dependencies at the abstract layers (interface) over the implementation layer.

That's where the protocol comes into play. The dataframe protocol is the interface that specifies a common representation of dataframes and thus restores the broken dependency inversion design pattern. 

So now, we will have a kind of `to_dataframe`/`from_dataframe` that allows us to go from a  dataframe object of a given library (supporting the protocol) to the protocol dataframe object which is a safe path to preserve library specific features. Also the protocol enforces zero-copy as much as possible which gets us rid of the possible memory overhead mentioned.


<br/>
<p align="center">
    <img
     alt="On the left, we have the interoperability between dataframe libraries through `pandas` which is a implementation dependency. On the right, we have the interoperability through the interchange dataframe API which an abstract dependency"
     src="/images/2021/10/dataframe-api-cudf/design_comparison.jpg">
    <i>Design comparison without and with the interchange dataframe protocol API </i>
</p>
<br/>


One of the main benefits is that each dataframe library can evolve independently as long as the interface contract specification is followed and we are free from any dataframe library dependency as is the case with pandas.
For more details about the purpose and scope of the protocol, please take a look [here](https://github.com/data-apis/dataframe-api/blob/main/protocol/purpose_and_scope.md)


### A brief description of the protocol dataframe interface

The interchange dataframe protocol interface is in fact a composition of interfaces:

<br/>

<p align="center">
    <img
     alt="A composition of the 3 interfaces forming the dataframe interchange protocol: `_CuDFDataFrame` has 1 or more `_CuDFColumn` which in turn has 1 or more `_CuDFBuffer` "
     src="/images/2021/10/dataframe-api-cudf/protocol_interfaces.jpg">
    <i>Composition of the interchange dataframe protocol interfaces</i>
</p>
<br/>
               
So each library supporting the protocol should implement these 3 interfaces that can be described as follows:

- **DataFrame** mainly exposes different methods to access/select columns (by name, index) and knowing the number of rows.

- **Column** has methods to access the column data type, describe  valid/missing values, exposes different buffers (data, validity and offset),  chunks, ...etc.

- **Buffer** has methods to describe the contiguous block of memory of the column data i.e device (GPU, CPU, ...), memory address, size, etc...

For more details, please have a look at [Python code of the protocol interfaces](https://github.com/data-apis/dataframe-api/blob/main/protocol/dataframe_protocol.py) and [design concepts](https://github.com/data-apis/dataframe-api/blob/main/protocol/design_requirements.md).

### What is expected from cudf implementation

Let's recap the protocol main features to be implemented in `cuDF`:


<table  style="width:100%">
  <tr>
    <th>Simple dtype </th>
    <th>Complex dtype </th>
    <th>Device</th>
    <th>Missing values</th>
    <th>Chunks</th>

  </tr>
  <tr>
    <td>int</td>
    <td>categorical</td>
    <td>GPU</td>
    <td>all dtypes (simple & complex)</td>
    <td>single</td>

  </tr>
  <tr>
    <td>uint8</td>
    <td>string</td>
    <td>CPU</td>
    <td>-</td>
    <td>multiple</td>

  </tr>
  <tr>
    <td>float</td>
    <td>datetime</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <br/>
</table> 

 <br/>

The above table shows that we should support the interchange dataframe protocol for cudf dataframes with column of various dtypes (simple and complex) and handle their missing values. Although, we aim at supporting data on GPU which is the case for cuDF, we ...

## What has been done or milestones

Checked elements in the table below represent implemented features so far.


<table  style="width:100%">
  <tr>
    <th>Simple dtype </th>
    <th>Complex dtype </th>
    <th>Device</th>
    <th>Missing values</th>
    <th>Chunks</th>

  </tr>
  <tr>
    <td><input type="checkbox" disabled="disabled" checked="checked"> int</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> categorical</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> GPU</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> all supported dtypes (simple & complex)</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> single</td>

  </tr>
  <tr>
    <td><input type="checkbox" disabled="disabled" checked="checked"> uint8</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> string</td>
    <td><input type="checkbox" disabled="disabled" checked="checked"> CPU</td>
    <td>-</td>
    <td><input type="checkbox" disabled="disabled" > multiple</td>

  </tr>
  <tr>
    <td> <input type="checkbox" disabled="disabled" checked="checked"> float</td>
    <td><input type="checkbox" disabled="disabled"> datetime</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td> <input type="checkbox" disabled="disabled" checked="checked"> bool</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <br/>
</table> 

 <br/>

We're still working on the `string` support. Note that we support CPU dataframes like pandas but since the protocol has not been integrated in the pandas repo, we can only test it locally. 
We've submitted this work as a [Pull Request](https://github.com/rapidsai/cudf/pull/9071) still under review, to rapidsai/cudf github repo.

Now, let's walk through some codes to see the protocol in action.
We start by creating a cudf dataframe object with columns named after supported dtypes:

```python
import cudf
import cupy as cp
data = {'int': [1000, 2, 300, None], 
        'uint8': cp.array([0, 128, 255, 25], dtype=cp.uint8),
        'float': [None, 2.5, None, 10],
        'bool': [True, None, False, True]}
        
df = cudf.DataFrame(data)
df['categorical'] = df['int'].astype('category')
```
Let's see the dataframe and make sure column dtypes are correctly recognized internally:
```python
print(f'{df} \n\n'); df.info()
```
**output**:

        int  uint8 float   bool categorical
    0  1000      0  <NA>   True        1000
    1     2    128   2.5   <NA>           2
    2   300    255  <NA>  False         300
    3  <NA>     25  10.0   True        <NA> 


    <class 'cudf.core.dataframe.DataFrame'>
    RangeIndex: 4 entries, 0 to 3
    Data columns (total 5 columns):
     #   Column       Non-Null Count  Dtype
    ---  ------       --------------  -----
     0   int          3 non-null      int64
     1   uint8        4 non-null      uint8
     2   float        2 non-null      float64
     3   bool         3 non-null      bool
     4   categorical  3 non-null      category
    dtypes: bool(1), category(1), float64(1), int64(1), uint8(1)
    memory usage: 356.0 bytes


Now, we create the interchange protocol dataframe object and check that basic information like number of rows, column names and dtypes are accurate:
```python
dfo =  df.__dataframe__()
print(f'{dfo}: {dfo.num_rows()} rows')
print('Column name\t Non-Null Count\t\t\t\t    Dtype')
print('-----------\t --------------\t\t\t\t    -----')
for n, c in zip(dfo.column_names(), dfo.get_columns()): 
    print(f'{n}\t\t\t      {int(c.size - c.null_count)}\t\t\t{c.dtype}')
```
**output**:

    <cudf.core.df_protocol._CuDFDataFrame object at 0x7ff90ac10ca0>: 4 rows
    Column name	 Non-Null Count				    Dtype
    -----------	 --------------				    -----
    int			          3		 (<_DtypeKind.INT: 0>, 64, '<i8', '=')
    uint8			      4		 (<_DtypeKind.UINT: 1>, 8, '|u1', '|')
    float			      2		 (<_DtypeKind.FLOAT: 2>, 64, '<f8', '=')
    bool			      3		 (<_DtypeKind.BOOL: 20>, 8, '|b1', '|')
    categorical		      3		 (<_DtypeKind.CATEGORICAL: 23>, 8, '|u1', '=')
    

How about buffers? We will examine those of the 'float' column:
```python
fcol = dfo.get_column_by_name('float')
buffers = fcol.get_buffers()
for k in buffers:
    print(f'{k}: {buffers[k]}\n')
```
**output**:

    data: (CuDFBuffer({'bufsize': 32, 'ptr': 140704936368128, 'dlpack': <capsule object "dltensor" at 0x7ff893505e40>, 'device': 'CUDA'}), (<_DtypeKind.FLOAT: 2>, 64, '<f8', '='))

    validity: (CuDFBuffer({'bufsize': 512, 'ptr': 140704936365568, 'dlpack': <capsule object "dltensor" at 0x7ff893505e40>, 'device': 'CUDA'}), (<_DtypeKind.UINT: 1>, 8, 'C', '='))

    offsets: None
    
We can notice the column dtype `<_DtypeKind.FLOAT: 2>` from the data buffer and the dtype of the validity mask which is always`<_DtypeKind.UINT: 1>` here. Finally there is no `offset` buffer as it is reserved to variable-length data like string.

Let's retrieve data and validity arrays from their buffers using the [DLPack protocol](https://github.com/dmlc/dlpack) and compare with the column itself:
```python
data_buffer = fcol.get_buffers()['data'][0]
validity_buffer = fcol.get_buffers()['validity'][0]
data = cp.fromDlpack(data_buffer.__dlpack__())
validity = cp.fromDlpack(validity_buffer.__dlpack__())
print(f'column: {df.float}')
print(f'data: {data}')
print(f'validity: {validity}')
```
**output**:

    float column
    0    <NA>
    1     2.5
    2    <NA>
    3    10.0
    Name: float, dtype: float64

    data: [ 0.   2.5  0.  10. ]
    validity: [0 1 0 1]

Comparing the float column and the data, we see that values are similar except `<NA>` in the column correspond to `0` in the data array. In fact, at the buffer level, we encode missing values by a 'sentinel value' which is 0 here. This is where the validity array comes into play. Together with the data array, we are able to rebuild the column with missing values in their exact places. How? 0s in the validity array indicates places or indexes of missing values in the data and 1s indicates valid/non-missing values.
All this work is done by a helper function `_from_dataframe` which builds up an entire cudf dataframe from an interchange dataframe object:

```python
from cudf.core.df_protocol import _from_dataframe
df_rebuilt = _from_dataframe(dfo)
print(f'rebuilt df\n----------\n{df_rebuilt}\n')
print(f'df\n--\n{df}')
```
**output**:

    rebuilt df
    ----------
        int  uint8 float   bool categorical
    0  1000      0  <NA>   True        1000
    1     2    128   2.5   <NA>           2
    2   300    255  <NA>  False         300
    3  <NA>     25  10.0   True        <NA>

    df
    --
        int  uint8 float   bool categorical
    0  1000      0  <NA>   True        1000
    1     2    128   2.5   <NA>           2
    2   300    255  <NA>  False         300
    3  <NA>     25  10.0   True        <NA>


We just went over a roundtrip demo from a cudf dataframe to the interchange dataframe object. Then we saw how to build a cudf dataframe object from the interchange dataframe object. Along the way, we've checked the integrity of the data.



## Lessons learned on the go

### Diversity advantage
Many studies show the benefits and better performance of diverse teams. My experience in this project was the [`CONTRIBUTING.md`](https://github.com/rapidsai/cudf/blob/branch-21.08/CONTRIBUTING.md)(old version) document on the repository which was very unclear for me as a newcomer. Following my mentors' advice ([Kshiteej Kalambarkar](https://github.com/kshitij12345) and [Ralf Gommers](https://github.com/rgommers)), I've opened an issue where I've shared my thoughts and kept asking clarifications which ended up in a (merged) [PR](https://github.com/rapidsai/cudf/pull/9026) to restructure the [`CONTRIBUTING.md`](https://github.com/rapidsai/cudf/blob/branch-21.12/CONTRIBUTING.md) (current version) document to make it clearer.

Thus, a diversity of levels (experts, newcomers, etc...) ensures an inclusive environment where everyone can find their way easily.

### Test Driven Development (TDD)
This process has been very helpful during this project. I've noticed my slowness on days where I've started developing features before writing any test. I kept going back and forth in the code base to ensure the coherence of different pieces of code written for that feature. However, when writing tests I felt the possibility to express all my expectations across different test cases then writing code for each one at a time. In this process, I felt like the tests pointed out next place on the code base where they might be something wrong. 

### Collaboration, speaking out is very helpful
Sometimes, when stumbling upon a problem, just speaking out or sharing the problem to someone else opens your eyes to a possible solution. This happened to me countless times when discussing with my mentor @Kshiteej and my colleague @Alenka whose project is very close to mine. Otherwise, external inputs combined with ours will definitely be better than ours alone. 

### Patience and Perseverence

It is well known that configuring and installing drivers to work with GPU is not an easy task. During this internship I've been rebuilding the cudf library many times and encountered multiple issues. I came up with a recipe which is: 

- seek help after around 40 minutes of debugging even if you have more ideas to try. 
- When asking for help, do share what you've tried and other ideas to try. 

Speaking out can reveal some gaps as previously mentioned.
When something is missing, make an attempt to fix it. If it went well, that'll be a contribution. That happened to me when trying to unpack bits with `bitorder='little'` as in `numpy`, using `cupy` or `cudf`. I've ended up submitting a (merged) [Pull Request to cupy](https://github.com/cupy/cupy/pull/5765).

## Final words
I am:

- very grateful to the team that works hard to bring up these internships in particular to my mentors [Kshiteej Kalambarkar](https://github.com/kshitij12345) and [Ralf Gommers](https://github.com/rgommers) as well as [Ashwin Srinath](https://github.com/shwina) from [rapidsai/cudf](https://github.com/rapidsai/cudf);

- very glad to take part of this 1st cohort of interns at Quansight Labs;

- very happy be part of this warm and welcoming environment;

- proud of what this environment helps me achieve. 